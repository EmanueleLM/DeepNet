# -*- coding: utf-8 -*-
"""
Created on Mon Nov 27 19:37:21 2017

@author: Emanuele

This module introduces one of the main features of the DeepNet core: the exploration of the space
    of the possible nets wrt a fixed problem with a genetic algorithm. We start with a population of 
    n cromosomes that are n neural nets with different topologies and we combine them with specific
    genetic operators in order to find the best suited for the problem
Please note that this code is not natively parallelized, but we aim at developing a module that can do
    training of the various nets in parallel. 
    The aim of this project (to me) is not to learn parallel computation from scratch, 
    but how to use genetic algorithm to generate good nets
"""

import numpy as np
import deepnet as dn
import crossover as cr
import mutation as mu

MAX_NUM_LAYERS = 10; # maximum number of layers that a generic net generated by this code can have (we use C like code for 'global variables')
MIN_NUM_LAYERS = 2; # maximum number of layers that a generic net generated by this code can have
MAX_NUM_NEURONS = 100; # maximum number of neurons a layer can have
MIN_NUM_NEURONS = 5; # minimum number of neurons a layer can have

crossovers_dict = {}; # dict for the various crossover methods
mutations_dict = {"random": mu.randomMutation}; # dict for the various mutations methods


# this function is used to generate a population of n neural networks
# you have just to specify the input size, the output size and how many neural net we want to generate
# takes as input
#   input_size, the number of inputs we have (i.e. the dimensions of the image of the function we want to optimize)
#   output_size, the number of outputs of the function we want to optimize
#   population_size, the number of neural net we want to use to find the best one
# returns
#   net_population, a list that contains the nets of the fresh new population
def randomPopulation(input_size, output_size, population_size):
    net_population = list(); # a list since the elements inside each net are eterogeneous
    for n in range(population_size):
        n_layers = np.random.randint(MIN_NUM_LAYERS, MAX_NUM_LAYERS);
        act_dict_size = len(dn.activations_dict); # size of the activations' dictionary
        loss_dict_size = len(dn.loss_dict); # size of the losses' dictionary
        layer = np.array([]);
        for i in range(n_layers-1):            
            layer = np.append(layer, np.array([np.random.randint(MIN_NUM_NEURONS, MAX_NUM_NEURONS), list(dn.activations_dict.keys())[np.random.randint(0, act_dict_size)]]));      
        layer = np.append(layer, np.array([output_size, list(dn.activations_dict.keys())[np.random.randint(0, act_dict_size)]])); # output layer
        net_population.append(dn.DeepNet(input_size, layer.reshape(n_layers, 2), list(dn.loss_dict.keys())[np.random.randint(0, loss_dict_size)], True)); # append an element of type DeepNet
    return net_population;

        
""" Test part """
population = randomPopulation(3, 10, 2);
cr.onePointCrossover(population[0], population[1], .5);
            