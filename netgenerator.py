# -*- coding: utf-8 -*-
"""
Created on Mon Nov 27 19:37:21 2017

@author: Emanuele

This module introduces one of the main features of the DeepNet core: the exploration of the space
    of the possible nets wrt a fixed problem with a genetic algorithm. We start with a population of 
    n cromosomes that are n neural nets with different topologies and we combine them with specific
    genetic operators in order to find the best suited for the problem
Please note that this code is not natively parallelized, but we aim at developing a module that can do
    training of the various nets in parallel. 
    The aim of this project (to me) is not to learn parallel computation from scratch, 
    but how to use genetic algorithm to generate good nets
"""

import numpy as np
import deepnet as dn
import crossover as cr
import mutation as mu
import selection as sel

MAX_NUM_LAYERS = 10; # maximum number of layers that a generic net generated by this code can have (we use C like code for 'global variables')
MIN_NUM_LAYERS = 2; # maximum number of layers that a generic net generated by this code can have
MAX_NUM_NEURONS = 100; # maximum number of neurons a layer can have
MIN_NUM_NEURONS = 5; # minimum number of neurons a layer can have



crossovers_dict = {"one-point": cr.onePointCrossover}; # dict for the various crossover methods
mutations_dict = {"random": mu.randomMutation}; # dict for the various mutations methods
selection_dict = {"rank": sel.rankSelection}; # dict for the various selection methods


# this function is used to generate a population of n neural networks
# you have just to specify the input size, the output size and how many neural net we want to generate
# takes as input
#   input_size, the number of inputs we have (i.e. the dimensions of the image of the function we want to optimize)
#   output_size, the number of outputs of the function we want to optimize
#   population_size, the number of neural net we want to use to find the best one
# returns
#   net_population, a list that contains the nets of the fresh new population
def randomPopulation(input_size, output_size, population_size):
    net_population = list(); # a list since the elements inside each net are eterogeneous
    for n in range(population_size):
        n_layers = np.random.randint(MIN_NUM_LAYERS, MAX_NUM_LAYERS);
        act_dict_size = len(dn.activations_dict); # size of the activations' dictionary
        loss_dict_size = len(dn.loss_dict); # size of the losses' dictionary
        layer = np.array([]);
        for i in range(n_layers-1):            
            layer = np.append(layer, np.array([np.random.randint(MIN_NUM_NEURONS, MAX_NUM_NEURONS), list(dn.activations_dict.keys())[np.random.randint(0, act_dict_size)]]));      
        layer = np.append(layer, np.array([output_size, list(dn.activations_dict.keys())[np.random.randint(0, act_dict_size)]])); # output layer
        net_population.append(dn.DeepNet(input_size, layer.reshape(n_layers, 2), list(dn.loss_dict.keys())[np.random.randint(0, loss_dict_size)], True)); # append an element of type DeepNet
    return net_population;

#this function returns, for a population of n nets, the fitness against a given problem
# in terms of accuracy and size of the net
#each net has its fitness evaluated in this way: the size of the net in terms of parameters, 
#    multiplied by a parameter w0 (gives a certain importance to this factor), plus the accuracy
#    times another parameter w1, such that (w0+w1)=1 (i.e. the fitness is a convex combination of the size of the net and the accuracy)
#    Obviously we should set w1>>w0 s.t. a little net with low accuracy cannot be better than a huge net with good accuracy
# return a list [[net], fitness]^+
# takes as input
#   population, a list that contains all the nets (not the couples <net, fitness>)
# returns
#   a list of tuples <net, fitness>, where each net has its own fitness calculated
def evaluateFitness(nets):
    population = list([[n, .0] for n in nets]); # put the couples <net, fitness>, initially fintess = 0 forall nets
    for n in range(len(nets)):
        population[n][1] = .0; # calculate the fitness, this part should be massively parallelized
    return population;
        
""" Test part """
nets = randomPopulation(3, 10, 5); # create the population 
population = evaluateFitness(nets);  
crossovers_dict["one-point"](nets[0], nets[1], 1); # apply crossover 
print(selection_dict["rank"](population, False));