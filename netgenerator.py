# -*- coding: utf-8 -*-
"""
Created on Mon Nov 27 19:37:21 2017

@author: Emanuele

This module introduces one of the main features of the DeepNet core: the exploration of the space
    of the possible nets wrt a fixed problem with a genetic algorithm. We start with a population of 
    n cromosomes that are n neural nets with different topologies and we combine them with specific
    genetic operators in order to find the best suited for the problem
Please note that this code is not natively parallelized, but we aim at developing a module that can do
    training of the various nets in parallel. 
    The aim of this project (to me) is not to learn parallel computation from scratch, 
    but how to use genetic algorithm to generate good nets
"""

import numpy as np
import deepnet as dn
import crossover as cr
import mutation as mu
import selection as sel

MAX_NUM_LAYERS = 3; # maximum number of layers that a generic net generated by this code can have (we use C like code for 'global variables')
MIN_NUM_LAYERS = 2; # maximum number of layers that a generic net generated by this code can have
MAX_NUM_NEURONS = 100; # maximum number of neurons a layer can have
MIN_NUM_NEURONS = 5; # minimum number of neurons a layer can have



crossovers_dict = {"one-point": cr.onePointCrossover}; # dict for the various crossover methods
mutations_dict = {"random": mu.randomMutation}; # dict for the various mutations methods
selection_dict = {"rank": sel.rankSelection}; # dict for the various selection methods


# this function is used to generate a population of n neural networks
# you have just to specify the input size, the output size and how many neural net we want to generate
# takes as input
#   input_size, the number of inputs we have (i.e. the dimensions of the image of the function we want to optimize)
#   output_size, the number of outputs of the function we want to optimize
#   population_size, the number of neural net we want to use to find the best one
# returns
#   net_population, a list that contains the nets of the fresh new population
def randomPopulation(input_size, output_size, population_size):
    net_population = list(); # a list since the elements inside each net are eterogeneous
    for n in range(population_size):
        n_layers = np.random.randint(MIN_NUM_LAYERS, MAX_NUM_LAYERS);
        act_dict_size = len(dn.activations_dict); # size of the activations' dictionary
        loss_dict_size = len(dn.loss_dict); # size of the losses' dictionary
        layer = np.array([]);
        for i in range(n_layers-1):            
            layer = np.append(layer, np.array([np.random.randint(MIN_NUM_NEURONS, MAX_NUM_NEURONS), list(dn.activations_dict.keys())[np.random.randint(0, act_dict_size)]]));      
        layer = np.append(layer, np.array([output_size, list(dn.activations_dict.keys())[np.random.randint(0, act_dict_size)]])); # output layer
        net_population.append(dn.DeepNet(input_size, layer.reshape(n_layers, 2), list(dn.loss_dict.keys())[np.random.randint(0, loss_dict_size)], True)); # append an element of type DeepNet
    return net_population;

#this function returns, for a population of n nets, the fitness against a given problem
# in terms of accuracy and size of the net
#each net has its fitness evaluated in this way: the size of the net in terms of parameters, 
#    multiplied by a parameter w0 (gives a certain importance to this factor), plus the accuracy
#    times another parameter w1, such that (w0+w1)=1 (i.e. the fitness is a convex combination of the size of the net and the accuracy)
#    Obviously we should set w1>>w0 s.t. a little net with low accuracy cannot be better than a huge net with good accuracy
# return a list [[net], fitness]^+
# takes as input
#   population, a list that contains all the nets (not the couples <net, fitness>)
# returns
#   a list of tuples <net, fitness>, where each net has its own fitness calculated
def evaluateFitness(nets):
    population = list([[n, .0] for n in nets]); # put the couples <net, fitness>, initially fintess = 0 forall nets
    num_parameters_per_net = np.array([n.numberOfParameters() for n in nets]); # extract for each net the number of parameters
    max_num_parameters = np.max(num_parameters_per_net); # extract the size of the biggest net
    for n in range(len(nets)):
        """ in this part we need to evaluate the fintess of a single network,
            this means that we need to test the net against a problem and extract
            the validation error. then we will calculate a measure of the size of the net
            and we use those two parameters in a convex combination to calculate the 
            fitness of each net """
        errors = (validateNet(population[n][0]));
        print(errors);
        population[n][1] = .9*errors + .1*(1-(num_parameters_per_net[n]/max_num_parameters)); # calculate the fitness, this part should be massively parallelized
    return population;

# function that validate the net i.e. calculate the validation error for a network
# in this case we use the 8x8 handwritten dataset in scikit-learn
#   we must implement a way to test it against a generic problem
def validateNet(net):
    for i in range(len(net.W)): #initialize the weights
        net.setWeights(dn.weights_dict['lecun'](net.W[i]), i); 
    import utils_digit_recognition as drec
    train_percentage = 60; # percentage of the dataset used for training
    validation_percentage = 20; # this percentage must be lower than the test set, since it's taken directly from it (for the sake of simplicity)
    digits = drec.load_digits(); # import the dataset
    images, targets = drec.unison_shuffled_copies(digits.images, digits.target); # shuffle together inputs and supervised outputs
    train, test = drec.dataSplit(images, train_percentage);# split train adn test
    train_Y, test_Y = drec.dataSplit(targets, train_percentage); # split train and test labels
    validation, test = drec.dataSplit(test, validation_percentage);
    validation_Y, test_Y = drec.dataSplit(test_Y, validation_percentage);
    train_Y = drec.binarization(train_Y); # binarize both the train and test labels
    test_Y = drec.binarization(test_Y); # ..
    validation_Y = drec.binarization(validation_Y);    
    X = train.reshape(train.shape[0], train.shape[1]*train.shape[2]).T;
    Y = train_Y;    
    X = drec.normalizeData(X);
    X_test = test.reshape(test.shape[0], test.shape[1]*test.shape[2]).T;
    Y_test = test_Y;    
    X_validation = validation.reshape(validation.shape[0], validation.shape[1]*validation.shape[2]).T;
    Y_validation = validation_Y;
    """ Train with full batch (size of the batch equals to the size of the dataset) """
    epochs = 3;
    validation_error = 1; # validation stop metric, initially the error is everywhere
    validation_size = X_validation.shape[1];
    for e in range(epochs):
        #print((epochs-e)," epochs left");
        for n in range(X.shape[1]):
            net.backpropagation(X[:,n].reshape(64,1), Y[n].reshape(10,1));
            number_of_errors_validation = 0;
        for n in range(X_validation.shape[1]):
            if np.argmax(net.netActivation(X_validation[:,n].reshape(64,1))) != np.argmax(Y_validation[n].reshape(10,1)):
                number_of_errors_validation += 1;
        if float(number_of_errors_validation/validation_size) > validation_error:
            break;
        else:
            validation_error = number_of_errors_validation/validation_size;
            #print("validation error: ", validation_error);
    """ test how much we are precise in our prediction """
    number_of_errors = 0; # total number of errors on the test set
    test_size = X_test.shape[1];
    for n in range(X_test.shape[1]):
        if np.argmax(net.netActivation(X_test[:,n].reshape(64,1))) != np.argmax(Y_test[n].reshape(10,1)):
            number_of_errors += 1;
    #print("The error percentage is ", number_of_errors/test_size, ": ", number_of_errors," errors out of ", test_size, " samples on test set.");
    return number_of_errors/test_size;
    
        
""" Test part """
nets = randomPopulation(64, 10, 5); # create the population 
population = evaluateFitness(nets);  
crossovers_dict["one-point"](nets[0], nets[1], 1); # apply crossover 